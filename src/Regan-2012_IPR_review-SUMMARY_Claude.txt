# COMPREHENSIVE SUMMARY WITH PYTHON IMPLEMENTATION GUIDE

## PAPER OVERVIEW
Title: Intrinsic Chess Ratings (by Kenneth W. Regan and Tamal Biswas, University at Buffalo)

The paper develops a method to measure chess playing strength independent of opponent ratings by analyzing move quality against computer evaluations. This allows testing whether Elo ratings have inflated over time.

## KEY SECTIONS AND PYTHON FORMULAS

### 1. CORE MODEL (Section 2)

The model assumes players choose moves probabilistically based on scaled value differences from the best move.

**Position Setup:**
- For position t, computer engine provides ordered moves with values v_0_t >= v_1_t >= v_2_t >= ...
- Define scaled differences: delta_i_t = (v_0_t - v_i_t) / s where s is sensitivity parameter
- Let p_0_t be probability of choosing best move (move 0)

**Core Probability Formula (Equation 4):**
```python
import numpy as np

# For move i at position t:
def compute_move_probability(i, delta_i_t, p_0_t, s, c):
    if i == 0:
        return p_0_t
    else:
        alpha = np.exp(-((delta_i_t / s) ** c))
        p_i_t = p_0_t ** alpha
        return p_i_t
```

**Key Quote:** "Here p_0,t is the probability of playing the computer's top-listed move in position t, and δ_i,t = (v_0,t − v_i,t)/s is the scaled difference in value between the top move and the i-th move. The parameter s > 0 is called the sensitivity and c > 0 the consistency."

### 2. EXPECTED ERROR METRICS (Equation 5)

**Match Percentage (MM_e):**
```python
def compute_MM_e(positions, p_0_values):
    """Expected percentage of matches to computer's first move"""
    T = len(positions)
    MM_e = sum(p_0_values) / T * 100  # as percentage
    sigma_MM_e = np.sqrt(sum(p_0_t * (1 - p_0_t) for p_0_t in p_0_values)) / T * 100
    return MM_e, sigma_MM_e
```

**Average Error (AE_e):**
```python
def compute_AE_e(positions, move_probs, deltas):
    """Expected average error per move
    positions: list of T positions
    move_probs: dict mapping (t, i) to p_i_t
    deltas: dict mapping (t, i) to delta_i_t
    """
    T = len(positions)
    
    # Sum over all positions and moves i >= 1
    total_error = 0
    variance_sum = 0
    
    for t in range(T):
        position_error = 0
        position_variance = 0
        
        for i in range(1, len(positions[t]['moves'])):  # i >= 1
            p_i_t = move_probs[(t, i)]
            delta_i_t = deltas[(t, i)]
            
            position_error += p_i_t * delta_i_t
            position_variance += p_i_t * (1 - p_i_t) * delta_i_t
        
        total_error += position_error
        variance_sum += position_variance
    
    AE_e = total_error / T
    sigma_AE_e = np.sqrt(variance_sum / T)
    
    return AE_e, sigma_AE_e
```

**Key Quote:** "The value is the expected error per move on the union of the training sets. We denote it by AE_e, and note that it, the expected number MM_e of matches to the computer's first-listed move, and projected standard deviations for these two quantities, are given by these formulas: MM_e = sum(p_0_t for t in 1..T), sigma_MM_e = sqrt(sum(p_0_t*(1-p_0_t) for t in 1..T)), AE_e = (1/T)*sum(sum(p_i_t*delta_i_t for i>=1) for t in 1..T), sigma_AE_e = sqrt((1/T)*sum(sum(p_i_t*(1-p_i_t)*delta_i_t for i>=1) for t in 1..T))"

### 3. PARAMETER FITTING WORKFLOW

**Step 1: Create Training Sets**
```python
def create_training_set(games_database, elo_target, elo_tolerance=10, 
                        year_start=2006, year_end=2009):
    """Extract games where both players rated within tolerance of elo_target
    
    Args:
        games_database: collection of chess games with ratings
        elo_target: target Elo (e.g., 2700, 2600, 2500)
        elo_tolerance: +/- range (default 10)
        year_start, year_end: time period
    
    Returns:
        List of positions from qualifying games
    """
    training_positions = []
    
    for game in games_database:
        if (year_start <= game.year <= year_end and
            abs(game.white_elo - elo_target) <= elo_tolerance and
            abs(game.black_elo - elo_target) <= elo_tolerance):
            
            # Extract positions from this game
            training_positions.extend(game.positions)
    
    return training_positions
```

**Step 2: Analyze Positions with Engine**
```python
def analyze_position_with_engine(position, engine, depth=13, multiPV=5):
    """Get top moves and their values from chess engine
    
    Args:
        position: chess position (FEN string or board object)
        engine: chess engine (e.g., Rybka 3, Stockfish)
        depth: search depth (paper uses depth 13)
        multiPV: number of top moves to analyze
    
    Returns:
        List of (move, value_in_pawns) tuples, ordered by value
    """
    # This is pseudocode - actual implementation depends on engine interface
    analysis = engine.analyze(position, depth=depth, multiPV=multiPV)
    
    moves_with_values = []
    for pv in analysis:
        move = pv['move']
        value = pv['score'] / 100  # convert centipawns to pawns
        moves_with_values.append((move, value))
    
    # Sort by value descending
    moves_with_values.sort(key=lambda x: x[1], reverse=True)
    
    return moves_with_values
```

**Step 3: Fit Parameters (s, c) to Training Data**
```python
from scipy.optimize import minimize

def fit_parameters(training_positions, actual_moves_played):
    """Fit s and c parameters to maximize likelihood of observed moves
    
    Args:
        training_positions: list of analyzed positions with engine moves/values
        actual_moves_played: list of actual moves played in each position
    
    Returns:
        (s_optimal, c_optimal, p_0_optimal)
    """
    
    def negative_log_likelihood(params):
        s, c, p_0 = params
        
        if s <= 0 or c <= 0 or p_0 <= 0 or p_0 >= 1:
            return 1e10  # invalid parameters
        
        log_likelihood = 0
        
        for t, position in enumerate(training_positions):
            engine_moves = position['engine_moves']  # [(move, value), ...]
            actual_move = actual_moves_played[t]
            
            # Find which engine move matches actual move
            move_index = None
            for i, (move, value) in enumerate(engine_moves):
                if move == actual_move:
                    move_index = i
                    break
            
            if move_index is None:
                continue  # move not in top engine moves
            
            # Compute probability of this move
            if move_index == 0:
                prob = p_0
            else:
                v_0 = engine_moves[0][1]
                v_i = engine_moves[move_index][1]
                delta_i = (v_0 - v_i) / s
                alpha = np.exp(-((delta_i / s) ** c))
                prob = p_0 ** alpha
            
            log_likelihood += np.log(prob + 1e-10)  # avoid log(0)
        
        return -log_likelihood
    
    # Initial guess
    initial_params = [0.15, 1.5, 0.5]  # s, c, p_0
    
    # Optimize
    result = minimize(negative_log_likelihood, initial_params, 
                     method='Nelder-Mead')
    
    s_opt, c_opt, p_0_opt = result.x
    return s_opt, c_opt, p_0_opt
```

**Key Quote:** "For each training set, they performed statistical regression to find the skill parameters s (sensitivity) and c (consistency) that best fit the actual move choices observed in those games."

**Step 4: Compute AE_e on Reference Set**
```python
def compute_AE_e_for_parameters(s, c, p_0, reference_set):
    """Compute expected average error on reference set R
    
    Args:
        s, c, p_0: fitted parameters
        reference_set: fixed set of positions (union of training sets or R)
    
    Returns:
        AE_e value
    """
    T = len(reference_set)
    total_error = 0
    
    for t, position in enumerate(reference_set):
        engine_moves = position['engine_moves']
        v_0 = engine_moves[0][1]  # best move value
        
        position_error = 0
        for i in range(1, len(engine_moves)):
            v_i = engine_moves[i][1]
            delta_i = v_0 - v_i  # unscaled difference in pawns
            delta_i_scaled = delta_i / s
            
            # Compute probability
            alpha = np.exp(-((delta_i_scaled) ** c))
            p_i = p_0 ** alpha
            
            position_error += p_i * delta_i  # use unscaled delta for error
        
        total_error += position_error
    
    AE_e = total_error / T
    return AE_e
```

### 4. ELO TO IPR CONVERSION (Equation 6)

**Key Quote:** "A simple linear fit to the Elo-AEe correspondence yielded equation (6): IPR = 3571 - 15413 × AE_e"

```python
def AE_e_to_IPR(AE_e):
    """Convert average error to Intrinsic Performance Rating"""
    IPR = 3571 - 15413 * AE_e
    return IPR

def IPR_to_AE_e(IPR):
    """Convert IPR back to average error"""
    AE_e = (3571 - IPR) / 15413
    return AE_e
```

**Table 1 Data (2006-2009):**
```python
# Elo -> AE_e correspondence from fitting 2006-2009 data
ELO_TO_AE_TABLE = {
    2700: 0.0572,
    2600: 0.0624,
    2500: 0.0686,
    2400: 0.0762,
    2300: 0.0857,
    2200: 0.0979
}

# Verify linear relationship
for elo, ae_e in ELO_TO_AE_TABLE.items():
    ipr = AE_e_to_IPR(ae_e)
    print(f"Elo {elo} -> AE_e {ae_e:.4f} -> IPR {ipr:.0f}")
```

### 5. COMPLETE WORKFLOW IMPLEMENTATION

```python
class IntrinsicRatingSystem:
    def __init__(self, engine, reference_set):
        self.engine = engine
        self.reference_set = reference_set
        self.elo_to_params = {}  # cache fitted parameters
    
    def create_training_sets(self, games_db, elo_levels, time_periods):
        """Create training sets for multiple Elo levels and time periods"""
        training_sets = {}
        
        for elo in elo_levels:
            for period_name, (year_start, year_end) in time_periods.items():
                key = (elo, period_name)
                training_sets[key] = create_training_set(
                    games_db, elo, year_start=year_start, year_end=year_end
                )
        
        return training_sets
    
    def fit_all_parameters(self, training_sets):
        """Fit parameters for all training sets"""
        results = {}
        
        for key, positions in training_sets.items():
            elo, period = key
            
            # Extract actual moves played
            actual_moves = [pos['actual_move'] for pos in positions]
            
            # Fit parameters
            s, c, p_0 = fit_parameters(positions, actual_moves)
            
            # Compute AE_e on reference set
            AE_e = compute_AE_e_for_parameters(s, c, p_0, self.reference_set)
            
            # Convert to IPR
            IPR = AE_e_to_IPR(AE_e)
            
            results[key] = {
                's': s,
                'c': c,
                'p_0': p_0,
                'AE_e': AE_e,
                'IPR': IPR,
                'nominal_elo': elo
            }
            
            print(f"Elo {elo} ({period}): s={s:.4f}, c={c:.4f}, "
                  f"AE_e={AE_e:.4f}, IPR={IPR:.0f}")
        
        return results
    
    def analyze_player_game(self, game_positions, actual_moves):
        """Analyze a player's game to compute their IPR
        
        Args:
            game_positions: list of positions from the game
            actual_moves: list of moves actually played
        
        Returns:
            Dictionary with performance metrics
        """
        # Analyze each position with engine
        analyzed_positions = []
        for pos in game_positions:
            engine_analysis = analyze_position_with_engine(pos, self.engine)
            analyzed_positions.append({
                'position': pos,
                'engine_moves': engine_analysis
            })
        
        # Fit parameters to this game
        s, c, p_0 = fit_parameters(analyzed_positions, actual_moves)
        
        # Compute AE_e on reference set
        AE_e = compute_AE_e_for_parameters(s, c, p_0, self.reference_set)
        
        # Convert to IPR
        IPR = AE_e_to_IPR(AE_e)
        
        # Also compute actual error in this game
        actual_error = 0
        matches = 0
        for i, pos_data in enumerate(analyzed_positions):
            best_move = pos_data['engine_moves'][0][0]
            best_value = pos_data['engine_moves'][0][1]
            actual_move = actual_moves[i]
            
            if actual_move == best_move:
                matches += 1
            else:
                # Find value of actual move
                for move, value in pos_data['engine_moves']:
                    if move == actual_move:
                        actual_error += (best_value - value)
                        break
        
        return {
            's': s,
            'c': c,
            'p_0': p_0,
            'AE_e': AE_e,
            'IPR': IPR,
            'actual_avg_error': actual_error / len(game_positions),
            'match_percentage': matches / len(game_positions) * 100
        }
```

### 6. KEY FINDINGS FROM PAPER

**Quote on Rating Inflation:** "The main finding is that for the 2006–2009 period, the Intrinsic Ratings agree with the Elo ratings to within the standard error of the data. For the 1991–1994 period there is slight but significant inflation, while for 1976–1979 the inflation is more marked."

**Table 1 Results (Elo vs IPR for different periods):**
```python
RESULTS_TABLE = {
    '2006-2009': {
        2700: {'AE_e': 0.0572, 'IPR': 2690, 'sigma': 9},
        2600: {'AE_e': 0.0624, 'IPR': 2609, 'sigma': 10},
        2500: {'AE_e': 0.0686, 'IPR': 2514, 'sigma': 11},
        2400: {'AE_e': 0.0762, 'IPR': 2397, 'sigma': 13},
        2300: {'AE_e': 0.0857, 'IPR': 2250, 'sigma': 15},
        2200: {'AE_e': 0.0979, 'IPR': 2062, 'sigma': 18}
    },
    '1991-1994': {
        2700: {'IPR': 2667, 'inflation': -33},
        2600: {'IPR': 2576, 'inflation': -24},
        2500: {'IPR': 2484, 'inflation': -16},
        2400: {'IPR': 2378, 'inflation': -22},
        2300: {'IPR': 2254, 'inflation': -46},
        2200: {'IPR': 2113, 'inflation': -87}
    },
    '1976-1979': {
        2700: {'IPR': 2589, 'inflation': -111},
        2600: {'IPR': 2519, 'inflation': -81},
        2500: {'IPR': 2444, 'inflation': -56},
        2400: {'IPR': 2357, 'inflation': -43},
        2300: {'IPR': 2249, 'inflation': -51},
        2200: {'IPR': 2119, 'inflation': -81}
    }
}
```

### 7. USAGE EXAMPLE

```python
# Initialize system
engine = initialize_chess_engine('stockfish')  # or Rybka 3
reference_set = load_reference_games()  # 2005, 2007 WC tournaments + 2006 match

rating_system = IntrinsicRatingSystem(engine, reference_set)

# Create training sets
elo_levels = [2700, 2600, 2500, 2400, 2300, 2200]
time_periods = {
    '2006-2009': (2006, 2009),
    '1991-1994': (1991, 1994),
    '1976-1979': (1976, 1979)
}

games_database = load_games_database()
training_sets = rating_system.create_training_sets(
    games_database, elo_levels, time_periods
)

# Fit parameters and compute IPRs
results = rating_system.fit_all_parameters(training_sets)

# Analyze a specific player's game
player_game = load_game('Kasparov_vs_Karpov_1985')
performance = rating_system.analyze_player_game(
    player_game.positions,
    player_game.moves
)

print(f"Player IPR: {performance['IPR']:.0f}")
print(f"Match %: {performance['match_percentage']:.1f}%")
print(f"Avg Error: {performance['actual_avg_error']:.3f} pawns")
```

### 8. IMPORTANT IMPLEMENTATION NOTES

**Engine Configuration:**
- Paper uses Rybka 3 at depth 13 with MultiPV=4 (top 4 moves)
- Positions analyzed: moves 8-60 of each game
- Skip positions with forced moves or where player in check

**Quote:** "We used the chess program Rybka 3 run to a uniform depth of 13 ply with the MultiPV option set to 4, meaning that it reports the top four moves it considers in the position."

**Data Filtering:**
```python
def should_include_position(position, move_number):
    """Determine if position should be included in analysis"""
    if move_number < 8 or move_number > 60:
        return False
    
    if position.is_check():
        return False
    
    # Skip if only one legal move
    if len(list(position.legal_moves)) == 1:
        return False
    
    return True
```

**Reference Set R:**
- 2005 FIDE World Championship (San Luis)
- 2006 World Championship Match (Kramnik vs Topalov)
- 2007 World Championship (Mexico City)

This provides a stable, high-quality reference for computing AE_e values across all time periods."}
